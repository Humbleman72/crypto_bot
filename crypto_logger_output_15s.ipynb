{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jKtLwsCCqsmH"
   },
   "source": [
    "Cryptocurrency trading bot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "# File:        cryptocurrency/crypto_logger_base.py\n",
    "# By:          Samuel Duclos\n",
    "# For          Myself\n",
    "# Description: Simple Binance logger base class.\n",
    "\n",
    "# Library imports.\n",
    "from cryptocurrency.resample import resample\n",
    "from binance.client import Client\n",
    "from abc import abstractmethod, ABC\n",
    "from time import sleep, time\n",
    "from os.path import exists, join\n",
    "from os import mkdir\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "class Crypto_logger_base(ABC):\n",
    "    def __init__(self, interval='15s', delay=4.7, buffer_size=3000, directory='crypto_logs', \n",
    "                 log_name='crypto_log', raw=False):\n",
    "        \"\"\"\n",
    "        :param interval: OHLCV interval to log. Default is 15 seconds.\n",
    "        :param delay: delay between Binance API requests. Minimum calculated was 4.7 seconds.\n",
    "        :param buffer_size: buffer size to avoid crashing on memory accesses.\n",
    "        :param directory: the directory where to output the logs.\n",
    "        :param log_name: name of the log file.\n",
    "        :param raw: whether the log dumps raw (instantaneous) or OHLCV data.\n",
    "        \"\"\"\n",
    "        self.interval = interval\n",
    "        self.delay = delay\n",
    "        self.buffer_size = buffer_size\n",
    "        self.directory = directory\n",
    "        self.raw = raw\n",
    "\n",
    "        self.log_name = join(self.directory, log_name + '.txt')\n",
    "        self.log_screened_name = join(self.directory, log_name + '_screened.txt')\n",
    "\n",
    "        if not exists(self.directory):\n",
    "            mkdir(self.directory)\n",
    "\n",
    "    #self.get_from_file(log_name=self.log_name, from_raw=False)\n",
    "    #self.get_from_file(log_name=self.input_log_name, from_raw=self.load_from_ohlcv)\n",
    "    def get_from_file(self, log_name, from_raw=False):\n",
    "        if from_raw:\n",
    "            dataset = pd.read_csv(log_name, header=0, index_col=0)\n",
    "        else:\n",
    "            dataset = pd.read_csv(log_name, header=[0, 1], index_col=0)\n",
    "        dataset.index = pd.DatetimeIndex(dataset.index)\n",
    "        return dataset.sort_index(axis='index')\n",
    "\n",
    "    @abstractmethod\n",
    "    def get(self, **kwargs):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    @abstractmethod\n",
    "    def screen(self, **kwargs):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def put(self, dataset):\n",
    "        dataset = dataset.copy().reset_index()\n",
    "        if self.raw:\n",
    "            dataset = dataset.drop_duplicates(subset=['symbol', 'count'], \n",
    "                                              keep='first', ignore_index=True)\n",
    "        else:\n",
    "            dataset = dataset.drop_duplicates(keep='last', ignore_index=True)\n",
    "\n",
    "        if 'date' in dataset.columns:\n",
    "            min_index_int = dataset[dataset['date'] == self.min_index].index[0]\n",
    "            dataset = dataset.set_index('date')\n",
    "        if not self.raw:\n",
    "            dataset = resample(dataset, self.interval)\n",
    "        if 'date' in dataset.columns:\n",
    "            dataset = dataset.iloc[min_index_int:]\n",
    "\n",
    "        dataset = dataset.tail(self.buffer_size)\n",
    "        dataset.to_csv(self.log_name)\n",
    "        self.min_index = dataset.index[0]\n",
    "        return dataset\n",
    "\n",
    "    def start(self, append=False, roll=0):\n",
    "        \"\"\"Main logger loop.\"\"\"\n",
    "        print('Starting crypto logger.')\n",
    "\n",
    "        if exists(self.log_name) and 'output' in self.log_name:\n",
    "            self.dataset = self.get_from_file(log_name=self.log_name, from_raw=False)\n",
    "            self.dataset = self.dataset.tail(self.buffer_size)\n",
    "        else:\n",
    "            self.dataset = self.get()\n",
    "\n",
    "        self.min_index = self.dataset.index[-1]\n",
    "        self.dataset = self.put(self.dataset)\n",
    "\n",
    "        while True:\n",
    "            try:\n",
    "                dataset = pd.concat([self.dataset, self.get()], axis='index', join='outer')\n",
    "            except (KeyboardInterrupt, SystemExit):\n",
    "                print('User terminated crypto logger process.')\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "            try:\n",
    "                self.dataset = self.put(dataset)\n",
    "            except (KeyboardInterrupt, SystemExit):\n",
    "                print('Saving latest complete dataset...')\n",
    "                self.dataset = self.put(dataset)\n",
    "                print('User terminated crypto logger process.')\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "            try:\n",
    "                if exists(self.log_screened_name):\n",
    "                    dataset_screened_old = \\\n",
    "                        pd.read_csv(self.log_screened_name, index_col=0, header=0)\n",
    "                else:\n",
    "                    dataset_screened_old = None\n",
    "                dataset_screened = self.screen(self.dataset)\n",
    "                if dataset_screened is not None:\n",
    "                    if roll != 0:\n",
    "                        if append and exists(self.log_screened_name):\n",
    "                            dataset_screened = \\\n",
    "                                pd.concat([dataset_screened_old, dataset_screened], axis='index')\n",
    "                            dataset_screened = \\\n",
    "                                dataset_screened.drop_duplicates(subset=['symbol'], keep='last')\n",
    "                        dataset_screened = dataset_screened.tail(roll)\n",
    "                        dataset_screened.to_csv(self.log_screened_name)\n",
    "                    elif append:\n",
    "                        dataset_screened.to_csv(self.log_screened_name, mode='a')\n",
    "                    else:\n",
    "                        dataset_screened.to_csv(self.log_screened_name)\n",
    "            except (KeyboardInterrupt, SystemExit):\n",
    "                print('User terminated crypto logger process.')\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "            sleep(self.delay)\n",
    "        print('Crypto logger process done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "# File:        cryptocurrency/crypto_logger_output.py\n",
    "# By:          Samuel Duclos\n",
    "# For          Myself\n",
    "# Description: Simple Binance logger output for arbitrary intervals.\n",
    "\n",
    "# Library imports.\n",
    "#from cryptocurrency.crypto_logger_base import Crypto_logger_base\n",
    "from cryptocurrency.indicators import filter_in_market, screen_one\n",
    "from os.path import exists, join\n",
    "\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "class Crypto_logger_output(Crypto_logger_base):\n",
    "    def __init__(self, delay=10, interval_input='15s', interval='15s', buffer_size=100, \n",
    "                 input_log_name='input'):\n",
    "        \"\"\"\n",
    "        :param delay: delay between Binance API requests. Minimum calculated was 4.7 seconds.\n",
    "        :param interval_input: OHLCV interval from input log. Default is 15 seconds.\n",
    "        :param interval: OHLCV interval to log. Default is 15 seconds.\n",
    "        :param buffer_size: buffer size to avoid crashing on memory accesses.\n",
    "        :param input_log_name: the directory where to take the logs from.\n",
    "        \"\"\"\n",
    "        self.data_before = pd.DataFrame()\n",
    "        input_log_name = 'crypto_' + input_log_name + '_log_'\n",
    "        self.load_from_ohlcv = interval_input != interval\n",
    "        super().__init__(interval=interval, delay=delay, buffer_size=buffer_size, \n",
    "                         directory='crypto_logs', log_name='crypto_output_log_' + interval, \n",
    "                         raw=False)\n",
    "\n",
    "        self.input_log_name = \\\n",
    "            join(self.directory, input_log_name + interval_input + '.txt')\n",
    "        self.input_log_screened_name = \\\n",
    "            join(self.directory, input_log_name + interval_input + '_screened.txt')\n",
    "\n",
    "    def screen(self, dataset):\n",
    "        if exists(self.input_log_screened_name):\n",
    "            input_filtered = pd.read_csv(self.input_log_screened_name, header=0, index_col=0)\n",
    "            input_filter = set(input_filtered['symbol'].tolist())\n",
    "            old_columns = set(dataset.columns.get_level_values(0).tolist())\n",
    "            new_columns = list(input_filter & old_columns)\n",
    "            dataset = dataset[new_columns]\n",
    "            #dataset.columns = dataset.columns.swaplevel(0, 1)\n",
    "            #dataset = dataset.rename(columns={'base_volume': 'volume'})\n",
    "            #dataset.columns = dataset.columns.swaplevel(0, 1)\n",
    "            assets = filter_in_market(screen_one, dataset)\n",
    "            #dataset.columns = dataset.columns.swaplevel(0, 1)\n",
    "            #dataset = dataset.rename(columns={'volume': 'base_volume'})\n",
    "            #dataset.columns = dataset.columns.swaplevel(0, 1)\n",
    "            return input_filtered[input_filtered['symbol'].isin(assets)]\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def resample_from_raw(self, df):\n",
    "        df = df[['symbol', 'close', 'rolling_base_volume', 'rolling_quote_volume']]\n",
    "        df['base_volume'] = df['rolling_base_volume'].copy()\n",
    "        df['quote_volume'] = df['rolling_quote_volume'].copy()\n",
    "        df = df.pivot_table(index=['date'], columns=['symbol'], \n",
    "                            values=['close', 'rolling_base_volume', \n",
    "                                    'rolling_quote_volume', \n",
    "                                    'base_volume', 'quote_volume'], \n",
    "                            aggfunc={'close': ['first', 'max', 'min', 'last'], \n",
    "                                     'base_volume': 'max', 'quote_volume': 'max', \n",
    "                                     'rolling_base_volume': 'max', \n",
    "                                     'rolling_quote_volume': 'max'})\n",
    "        df.columns = pd.MultiIndex.from_tuples([('_'.join(col[:2]), col[2]) for col in df.columns.values], \n",
    "                                               names=('pair', 'symbol'))\n",
    "        df = df.rename(columns={'close_first': 'open', 'close_max': 'high', \n",
    "                                'close_min': 'low', 'close_last': 'close', \n",
    "                                'base_volume_max': 'base_volume', \n",
    "                                'quote_volume_max': 'quote_volume', \n",
    "                                'rolling_base_volume_max': 'rolling_base_volume', \n",
    "                                'rolling_quote_volume_max': 'rolling_quote_volume'}, \n",
    "                       level=0)\n",
    "        df['base_volume'] = df['base_volume'].fillna(method='pad')\n",
    "        df['base_volume'].iloc[0] = 0\n",
    "        df['quote_volume'] = df['quote_volume'].fillna(method='pad')\n",
    "        df['quote_volume'].iloc[0] = 0\n",
    "        df['rolling_base_volume'] = df['rolling_base_volume'].fillna(method='pad')\n",
    "        df['rolling_base_volume'].iloc[0] = 0\n",
    "        df['rolling_quote_volume'] = df['rolling_quote_volume'].fillna(method='pad')\n",
    "        df['rolling_quote_volume'].iloc[0] = 0\n",
    "        df = df.sort_index().iloc[1:]\n",
    "        df.columns = df.columns.swaplevel(0, 1)\n",
    "        return df\n",
    "\n",
    "    def get(self):\n",
    "        dataset = self.get_from_file(log_name=self.input_log_name, \n",
    "                                     from_raw=not self.load_from_ohlcv)\n",
    "        if not self.load_from_ohlcv:\n",
    "            dataset = self.resample_from_raw(dataset)\n",
    "        return dataset.tail(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "# File:        crypto_logger_output_1min.py\n",
    "# By:          Samuel Duclos\n",
    "# For          Myself\n",
    "# Description: Simple Binance logger output for the 1 minute interval.\n",
    "\n",
    "# Library imports.\n",
    "#from cryptocurrency.crypto_logger_output import Crypto_logger_output\n",
    "\n",
    "crypto_logger_output_1min = Crypto_logger_output(delay=44, \n",
    "                                                 interval_input='15s', \n",
    "                                                 interval='1min', \n",
    "                                                 buffer_size=3000, \n",
    "                                                 input_log_name='output')\n",
    "crypto_logger_output_1min.start(append=False, roll=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "input_log_name = 'crypto_logs/crypto_input_log_15s.txt'\n",
    "df = pd.read_csv(input_log_name, header=0, index_col=0)\n",
    "df.index = pd.DatetimeIndex(df.index)\n",
    "df = df.sort_index(axis='index')\n",
    "#df = crypto_logger_output_1min.resample_from_raw(df)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#crypto_output_log_1d = crypto_logger_output_1d.log_name\n",
    "crypto_output_log_1d = 'crypto_logs/crypto_output_log_1d.txt'\n",
    "df_1d = pd.read_csv(crypto_output_log_1d, header=[0, 1], index_col=0)\n",
    "df_1d.index = pd.DatetimeIndex(df_1d.index)\n",
    "df_1d = df_1d.sort_index(axis='index')\n",
    "df_1d['BTC'].tail(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#crypto_output_log_1h = crypto_logger_output_1h.log_name\n",
    "crypto_output_log_1h = 'crypto_logs/crypto_output_log_1h.txt'\n",
    "df_1h = pd.read_csv(crypto_output_log_1h, header=[0, 1], index_col=0)\n",
    "df_1h.index = pd.DatetimeIndex(df_1h.index)\n",
    "df_1h = df_1h.sort_index(axis='index')\n",
    "df_1h['BTC'].tail(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#crypto_output_log_1min = crypto_logger_output_1min.log_name\n",
    "crypto_output_log_1min = 'crypto_logs/crypto_output_log_1min.txt'\n",
    "df_1min = pd.read_csv(crypto_output_log_1min, header=[0, 1], index_col=0)\n",
    "df_1min.index = pd.DatetimeIndex(df_1min.index)\n",
    "df_1min = df_1min.sort_index(axis='index')\n",
    "df_1min['BTC'].tail(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = crypto_logger_output_1min.dataset\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['BTCUSDT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['ZILBUSD']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas_ta as ta\n",
    "\n",
    "def filter_in_market(function, dataset):\n",
    "    def f(x):\n",
    "        x = x.loc[:,~x.columns.duplicated()]\n",
    "        return function(x)\n",
    "    tickers_list = dataset.columns.get_level_values(0).unique().tolist()\n",
    "    return pd.Series([ticker for ticker in tickers_list if f(dataset[ticker])], dtype='str')\n",
    "\n",
    "def get_relative_volume_levels_smoothed_trigger(data, average1=26, average2=14):\n",
    "    volume = data['volume']\n",
    "    volume_average = ta.sma(close=volume, length=average1)\n",
    "    relative_volume = volume / volume_average\n",
    "    smoothed_relative_volume = ta.sma(close=relative_volume, length=average2)\n",
    "    return (smoothed_relative_volume > threshold).iat[-1]\n",
    "\n",
    "def get_relative_volume_levels_at_time_smoothed_thresholded(data):\n",
    "    try:\n",
    "        volume = data['volume']\n",
    "        #volume = volume.groupby(pd.Grouper(freq='D')).cumsum()\n",
    "        cum_volume = volume.groupby(pd.Grouper(freq='24h')).cumsum()\n",
    "        #volume = volume.groupby(pd.Grouper(freq='60m')).cumsum()\n",
    "        cum_rvol = (cum_volume / cum_volume.shift(1)).fillna(method='pad')\n",
    "        rvol = (volume / volume.shift(1)).fillna(method='pad')\n",
    "        bar_up = (data['close'] > data['open'])\n",
    "        bar_up |= (data['close'] == data['open']) & (data['close'].diff() > 0)\n",
    "        bar_up = bar_up.astype(int)\n",
    "        bar_up = bar_up * 2 - 1\n",
    "        #rvol *= bar_up\n",
    "        cum_rvol_dir = cum_rvol * bar_up\n",
    "        rvol_dir = rvol * bar_up\n",
    "        rvol_indicator = ta.hma(rvol, length=14, talib=True)\n",
    "        rvol_dir_indicator = ta.hma(rvol_dir, length=14, talib=True)\n",
    "        cum_rvol_indicator = ta.hma(cum_rvol, length=14, talib=True)\n",
    "        cum_rvol_dir_indicator = ta.hma(cum_rvol_dir, length=14, talib=True)\n",
    "        rvol_indicator = rvol_indicator.rename('relative_volume_levels_smoothed')\n",
    "        rvol_dir_indicator = rvol_dir_indicator.rename('relative_volume_levels_dir_smoothed')\n",
    "        cum_rvol_indicator = cum_rvol_indicator.rename('cum_relative_volume_levels_smoothed')\n",
    "        cum_rvol_dir_indicator = cum_rvol_dir_indicator.rename('cum_relative_volume_levels_dir_smoothed')\n",
    "        #threshold = (ta.sma(rvol, length=100, talib=True) + ta.stdev(rvol, length=100, talib=True))\n",
    "        threshold_dir = 0\n",
    "        threshold = 2\n",
    "        rvol_thresholded = (rvol_indicator > threshold).iat[-1]\n",
    "        rvol_dir_thresholded = (rvol_dir_indicator > threshold_dir).iat[-1]\n",
    "        cum_rvol_thresholded = (cum_rvol_indicator > threshold).iat[-1]\n",
    "        cum_rvol_dir_thresholded = (cum_rvol_dir_indicator > threshold_dir).iat[-1]\n",
    "        trigger = (rvol_thresholded | rvol_dir_thresholded | cum_rvol_thresholded | cum_rvol_dir_thresholded)\n",
    "    except Exception as e:\n",
    "        print('rvol exception:', e)\n",
    "        trigger = False\n",
    "    return trigger\n",
    "\n",
    "def get_positive_trend_strength_trigger(data):\n",
    "    ADX = data.ta.adx(talib=True)\n",
    "    return (ADX['ADX_14'] < 0.20).iloc[-3] & (ADX['ADX_14'] > 0.20).iat[-2]\n",
    "\n",
    "def get_not_negative_trend_strength_trigger(data):\n",
    "    ADX = data.ta.adx(length=14, lensig=8, talib=True)\n",
    "    return ((ADX['DMP_14'] > ADX['DMN_14']) & (ADX['ADX_14'] > 0.30)).iat[-1]\n",
    "\n",
    "def get_not_negative_rebound_trigger(data):\n",
    "    CCI = data.ta.cci(length=22, talib=True)\n",
    "    MFI = data.ta.mfi(length=11, talib=True)\n",
    "    return ((CCI > 0) | (MFI > 20)).iat[-1]\n",
    "\n",
    "def get_positive_choppiness_trigger(data):\n",
    "    CHOP = data.ta.chop(talib=True)\n",
    "    return CHOP.iat[-1] < 38.2\n",
    "\n",
    "def get_positive_phase_trigger(data):\n",
    "    MACD = data.ta.macd(talib=True)\n",
    "    histogram = MACD['MACDs_12_26_9'] - MACD['MACD_12_26_9']\n",
    "    return ((histogram > histogram.shift(1)) | \\\n",
    "            (MACD['MACD_12_26_9'] > MACD['MACDs_12_26_9'])).iat[-1]\n",
    "\n",
    "def get_positive_phase_trigger(data):\n",
    "    MACD = data.ta.macd(talib=True)\n",
    "    histogram = MACD['MACDs_12_26_9'] - MACD['MACD_12_26_9']\n",
    "    return ((histogram.iloc[-2] > histogram.iat[-2]) or \\\n",
    "            (MACD['MACD_12_26_9'].iat[-1] > MACD['MACDs_12_26_9'].iat[-1]))\n",
    "\n",
    "def get_not_square_wave_triggers(data, multiplier_schedule):\n",
    "    triggers = True\n",
    "    for multiplier in multiplier_schedule:\n",
    "        period_1 = -4 * multiplier\n",
    "        uniques_1 = 2 * multiplier\n",
    "        square_wave_trigger_1 = (data.iloc[period_1:]['close'].unique().size < uniques_1)\n",
    "        if square_wave_trigger_1:\n",
    "            triggers = False\n",
    "            break\n",
    "        else:\n",
    "            period_2 = -15 * multiplier\n",
    "            uniques_2 = 6 * multiplier\n",
    "            square_wave_trigger_2 = (data.iloc[period_2:]['close'].unique().size < uniques_2)\n",
    "            if square_wave_trigger_2:\n",
    "                triggers = False\n",
    "                break\n",
    "    return triggers\n",
    "\n",
    "def get_minute_not_square_wave_triggers(data):\n",
    "    return get_not_square_wave_triggers(data, multiplier_schedule=[1, 2, 3, 5, 10, 15, 20, 45])\n",
    "\n",
    "def get_hourly_not_square_wave_triggers(data):\n",
    "    return get_not_square_wave_triggers(data, multiplier_schedule=[1, 2, 3, 4, 6, 8, 12])\n",
    "\n",
    "def get_daily_not_square_wave_triggers(data):\n",
    "    return get_not_square_wave_triggers(data, multiplier_schedule=[1])\n",
    "\n",
    "def get_daily_volume_minimum_trigger(data):\n",
    "    return (data['volume'] > 1000000).iat[-1]\n",
    "\n",
    "def get_daily_volume_change_trigger(data):\n",
    "    return ((data['volume'].pct_change(1) * 100) > 300).iat[-1]\n",
    "\n",
    "def get_minute_daily_volume_minimum_trigger(data):\n",
    "    return (data['rolling_base_volume'] > 1000000).iat[-1]\n",
    "\n",
    "def get_minute_daily_volume_change_trigger(data):\n",
    "    return ((data['rolling_base_volume'].pct_change(1440) * 100) > 300).iat[-1]\n",
    "\n",
    "def get_rising_volume_trigger(data):\n",
    "    return (data['rolling_base_volume'].diff(1) > 0).iat[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_1d.copy()\n",
    "df.columns = df.columns.swaplevel(0, 1)\n",
    "df = df.rename(columns={'base_volume': 'volume'})\n",
    "df.columns = df.columns.swaplevel(0, 1)\n",
    "filtered_1 = set(filter_in_market(get_daily_not_square_wave_triggers, df).tolist())\n",
    "filtered_2 = set(filter_in_market(get_daily_volume_minimum_trigger, df).tolist())\n",
    "filtered_3 = set(filter_in_market(get_relative_volume_levels_smoothed_trigger, df).tolist())\n",
    "filtered_1d = pd.Series(list(filtered_1 & filtered_2 & filtered_3))\n",
    "filtered_1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_1h.copy()\n",
    "df.columns = df.columns.swaplevel(0, 1)\n",
    "df = df.rename(columns={'base_volume': 'volume'})\n",
    "df.columns = df.columns.swaplevel(0, 1)\n",
    "filtered_1 = set(filter_in_market(get_hourly_not_square_wave_triggers, df).tolist())\n",
    "filtered_2 = set(filter_in_market(get_relative_volume_levels_at_time_smoothed_thresholded, df).tolist())\n",
    "filtered_1h = pd.Series(list(filtered_1 & filtered_2))\n",
    "filtered_1h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_1min.copy()\n",
    "df.columns = df.columns.swaplevel(0, 1)\n",
    "df = df.rename(columns={'base_volume': 'volume'})\n",
    "df.columns = df.columns.swaplevel(0, 1)\n",
    "filtered_1 = set(filter_in_market(get_minute_not_square_wave_triggers, df).tolist())\n",
    "filtered_2 = set(filter_in_market(get_minute_daily_volume_minimum_trigger, df).tolist())\n",
    "filtered_3 = set(filter_in_market(get_minute_daily_volume_change_trigger, df).tolist())\n",
    "filtered_4 = set(filter_in_market(get_rising_volume_trigger, df).tolist())\n",
    "filtered_1min = pd.Series(list(filtered_1 & filtered_2 & filtered_3 & filtered_4))\n",
    "filtered_1min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(set(filtered_1h.tolist()) & set(filtered_1d.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(set(filtered_1min.tolist()) & set(filtered_1h.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(set(filtered_1min.tolist()) & set(filtered_1d.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(set(filtered_1min.tolist()) & set(filtered_1h.tolist()) & set(filtered_1d.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "crypto_output_log_15s = 'crypto_logs/crypto_output_log_15s.txt'\n",
    "df_15s = pd.read_csv(crypto_output_log_15s, header=[0, 1], index_col=0)\n",
    "df_15s.index = pd.DatetimeIndex(df_15s.index)\n",
    "df_15s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "crypto_exchange_info = 'crypto_logs/crypto_exchange_info.txt'\n",
    "exchange_info = pd.read_csv(crypto_exchange_info, header=0, index_col=0)\n",
    "exchange_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cryptocurrency.authentication import Cryptocurrency_authenticator\n",
    "from cryptocurrency.exchange import Cryptocurrency_exchange\n",
    "from cryptocurrency.conversion_table import get_conversion_table\n",
    "\n",
    "authenticator = Cryptocurrency_authenticator(use_keys=False, testnet=False)\n",
    "client = authenticator.spot_client\n",
    "exchange = Cryptocurrency_exchange(client=client, directory='crypto_logs')\n",
    "exchange_info = exchange.info\n",
    "\n",
    "#conversion_table = get_conversion_table(client=client, exchange_info=exchange_info)\n",
    "#conversion_table.sort_values(by='rolling_traded_volume', ascending=False).reset_index(drop=True).head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cryptocurrency.conversion import convert_price, get_base_asset_from_pair, get_quote_asset_from_pair\n",
    "from pandas import concat, DataFrame\n",
    "import time\n",
    "\n",
    "def get_conversion_table(client, exchange_info, as_pair=False):\n",
    "    \"\"\"\n",
    "    Fetches and prepares data used to calculate prices, volumes and other stats.\n",
    "    :param client: object from python-binance useful for calling client.get_ticker().\n",
    "    :param exchange_info: Pre-calculated exchange information on all tickers.\n",
    "    :return: pd.DataFrame containing all preprocessed conversion table info.\n",
    "    :column is_shorted: is the symbol made from inversion.\n",
    "    :column symbol: concatenated string made from base_asset and quote_asset.\n",
    "    :column shorted_symbol: symbol with inverted base_asset and quote_asset.\n",
    "    :column base_asset: asset on the left.\n",
    "    :column quote_asset: asset on the right.\n",
    "    :column price_change: (close - open).\n",
    "    :column price_change_percent: (((close - open) / open) * 100).\n",
    "    :column USDT_price_change: (USDT_close - USDT_open).\n",
    "    :column USDT_price_change_percent: (((USDT_close - USDT_open) / USDT_open) * 100).\n",
    "    :column weighted_average_price: weighted average price.\n",
    "    :column close_shifted: close price of the previous day.\n",
    "    :column open: open price of the day.\n",
    "    :column high: high price of the day.\n",
    "    :column low: low price of the day.\n",
    "    :column close: close price of the day.\n",
    "    :column last_volume: volume of the last price update.\n",
    "    :column USDT_bid_price: USDT-converted bid price.\n",
    "    :column USDT_ask_price: USDT-converted ask price.\n",
    "    :column USDT_bid_volume: USDT-converted bid volume.\n",
    "    :column USDT_ask_volume: USDT-converted ask volume.\n",
    "    :column bid_price: price of the bid.\n",
    "    :column bid_volume: volume of the bid at bid_price.\n",
    "    :column ask_price: price of the ask.\n",
    "    :column ask_volume: volume of the ask at ask_price.\n",
    "    :column rolling_base_volume: rolling_base_volume given by the API.\n",
    "    :column rolling_quote_volume: rolling_quote_volume given by the API.\n",
    "    :column open_time: close_time minus 24 hours.\n",
    "    :column close_time: time from epoch in milliseconds of the last price update.\n",
    "    :column first_ID: transaction ID from 1 day ago.\n",
    "    :column last_ID: latest transaction ID.\n",
    "    :column count: value calculated by subtracting first_ID from last_ID.\n",
    "    :column USDT_open: USDT-converted open price.\n",
    "    :column USDT_high: USDT-converted high price.\n",
    "    :column USDT_low: USDT-converted low price.\n",
    "    :column USDT_price: USDT-converted close price.\n",
    "    :column rolling_USDT_base_volume: USDT-converted rolling_base_volume.\n",
    "    :column rolling_USDT_quote_volume: USDT-converted rolling_quote_volume.\n",
    "    :column rolling_traded_volume: sum by base_asset of all USDT-converted volumes.\n",
    "    :column importance: rolling_USDT_base_volume divided by rolling_traded_volume.\n",
    "    :column traded_price: sum by base_asset of all (close prices times importance).\n",
    "    :column traded_bid_price: sum by base_asset of all (bid prices times importance).\n",
    "    :column traded_ask_price: sum by base_asset of all (ask prices times importance).\n",
    "    :column bid_ask_change_percent: ((ask_price - bid_price) / ask_price) * 100).\n",
    "    :column bid_ask_volume_percent: ((bid_volume / (bid_volume + ask_volume)) * 100).\n",
    "    :column traded_bid_ask_change_percent: ((traded_ask_price - traded_bid_price) / traded_ask_price) * 100).\n",
    "    :column traded_bid_ask_volume_percent: ((traded_bid_volume / (traded_bid_volume + traded_ask_volume)) * 100).\n",
    "    \"\"\"\n",
    "    conversion_table = DataFrame(client.get_ticker())\n",
    "\n",
    "    conversion_table = conversion_table[conversion_table['symbol'].isin(exchange_info['symbol'])]\n",
    "    conversion_table['base_asset'] = \\\n",
    "        conversion_table['symbol'].apply(lambda x: get_base_asset_from_pair(x, exchange_info=exchange_info))\n",
    "    conversion_table['quote_asset'] = \\\n",
    "        conversion_table['symbol'].apply(lambda x: get_quote_asset_from_pair(x, exchange_info=exchange_info))\n",
    "\n",
    "    conversion_table = \\\n",
    "        conversion_table.rename(columns={'openPrice': 'open', 'highPrice': 'high', 'lowPrice': 'low', \n",
    "                                         'lastPrice': 'close', 'volume': 'rolling_base_volume', \n",
    "                                         'quoteVolume': 'rolling_quote_volume', 'lastQty': 'last_volume', \n",
    "                                         'bidPrice': 'bid_price', 'askPrice': 'ask_price', \n",
    "                                         'bidQty': 'bid_volume', 'askQty': 'ask_volume', \n",
    "                                         'firstId': 'first_ID', 'lastId': 'last_ID', \n",
    "                                         'openTime': 'open_time', 'closeTime': 'close_time', \n",
    "                                         'prevClosePrice': 'close_shifted', \n",
    "                                         'weightedAvgPrice': 'weighted_average_price', \n",
    "                                         'priceChange': 'price_change', \n",
    "                                         'priceChangePercent': 'price_change_percent'})\n",
    "    conversion_table[['close_time', 'price_change_percent', 'open', 'high', 'low', 'close', 'close_shifted', \n",
    "                      'bid_price', 'ask_price', 'bid_volume', 'ask_volume', 'rolling_base_volume', \n",
    "                      'rolling_quote_volume', 'count']] = \\\n",
    "        conversion_table[['close_time', 'price_change_percent', 'open', 'high', 'low', 'close', 'close_shifted', \n",
    "                          'bid_price', 'ask_price', 'bid_volume', 'ask_volume', 'rolling_base_volume', \n",
    "                          'rolling_quote_volume', 'count']].astype(float)\n",
    "    conversion_table['close_time'] = conversion_table['close_time'].astype(int)\n",
    "\n",
    "    is_dst = time.localtime().tm_isdst\n",
    "    timezone = time.tzname[is_dst]\n",
    "    offset_s = time.altzone if is_dst else time.timezone\n",
    "    offset = (offset_s / 60 / 60)\n",
    "    conversion_table['close_time'] = (conversion_table['close_time'] + offset_s * 1000)\n",
    "\n",
    "    conversion_table['rolling_base_quote_volume'] = \\\n",
    "        conversion_table['rolling_quote_volume'] / conversion_table['close']\n",
    "    conversion_table['USDT_open'] = \\\n",
    "        conversion_table.apply(lambda x: convert_price(size=1, from_asset=x['base_asset'], to_asset='USDT', \n",
    "                                                       conversion_table=conversion_table, \n",
    "                                                       exchange_info=exchange_info, key='open'), axis='columns')\n",
    "    conversion_table['USDT_high'] = \\\n",
    "        conversion_table.apply(lambda x: convert_price(size=1, from_asset=x['base_asset'], to_asset='USDT', \n",
    "                                                       conversion_table=conversion_table, \n",
    "                                                       exchange_info=exchange_info, key='close'), axis='columns')\n",
    "    conversion_table['USDT_low'] = \\\n",
    "        conversion_table.apply(lambda x: convert_price(size=1, from_asset=x['base_asset'], to_asset='USDT', \n",
    "                                                       conversion_table=conversion_table, \n",
    "                                                       exchange_info=exchange_info, key='close'), axis='columns')\n",
    "    conversion_table['USDT_price'] = \\\n",
    "        conversion_table.apply(lambda x: convert_price(size=1, from_asset=x['base_asset'], to_asset='USDT', \n",
    "                                                       conversion_table=conversion_table, \n",
    "                                                       exchange_info=exchange_info), axis='columns')\n",
    "    conversion_table['rolling_USDT_base_volume'] = \\\n",
    "        conversion_table['rolling_base_volume'] * conversion_table['USDT_price']\n",
    "    conversion_table['rolling_USDT_quote_volume'] = \\\n",
    "        conversion_table['rolling_base_quote_volume'] * conversion_table['USDT_price']\n",
    "\n",
    "    conversion_table['USDT_bid_price'] = \\\n",
    "        conversion_table.apply(lambda x: convert_price(size=x['bid_price'], from_asset=x['base_asset'], \n",
    "                                                       to_asset='USDT', conversion_table=conversion_table, \n",
    "                                                       exchange_info=exchange_info), \n",
    "                               axis='columns')\n",
    "    conversion_table['USDT_ask_price'] = \\\n",
    "        conversion_table.apply(lambda x: convert_price(size=x['ask_price'], from_asset=x['base_asset'], \n",
    "                                                       to_asset='USDT', conversion_table=conversion_table, \n",
    "                                                       exchange_info=exchange_info), \n",
    "                               axis='columns')\n",
    "    conversion_table['USDT_bid_volume'] = \\\n",
    "        conversion_table['bid_volume'] * conversion_table['USDT_bid_price']\n",
    "    conversion_table['USDT_ask_volume'] = \\\n",
    "        conversion_table['ask_volume'] * conversion_table['USDT_ask_price']\n",
    "\n",
    "    conversion_table['USDT_price_change'] = \\\n",
    "        (conversion_table['USDT_price'] * conversion_table['USDT_open']) / \\\n",
    "        conversion_table['USDT_open']\n",
    "    conversion_table['USDT_price_change_percent'] = \\\n",
    "        ((conversion_table['USDT_price_change'] / conversion_table['USDT_open']) * 100)\n",
    "\n",
    "    conversion_table['is_shorted'] = False\n",
    "\n",
    "    conversion_table_swapped = conversion_table.copy()\n",
    "    conversion_table_swapped.loc[:, ['symbol', 'price_change', 'price_change_percent', \n",
    "                                     'weighted_average_price', 'close_shifted', 'close', \n",
    "                                     'last_volume', 'ask_price', 'ask_volume', 'bid_price', \n",
    "                                     'bid_volume', 'open', 'high', 'low', 'rolling_quote_volume', \n",
    "                                     'rolling_base_volume', 'open_time', 'close_time', 'first_ID', \n",
    "                                     'last_ID', 'count', 'quote_asset', 'base_asset', \n",
    "                                     'rolling_base_quote_volume', 'USDT_open', 'USDT_high', \n",
    "                                     'USDT_low', 'USDT_price', 'rolling_USDT_quote_volume', \n",
    "                                     'rolling_USDT_base_volume', 'USDT_ask_price', 'USDT_bid_price', \n",
    "                                     'USDT_ask_volume', 'USDT_bid_volume', 'USDT_price_change', \n",
    "                                     'USDT_price_change_percent', 'is_shorted']] = \\\n",
    "        conversion_table_swapped.loc[:, ['symbol', 'price_change', 'price_change_percent', 'weighted_average_price', \n",
    "                                         'close_shifted', 'close', 'last_volume', 'bid_price', 'bid_volume', \n",
    "                                         'ask_price', 'ask_volume', 'open', 'high', 'low', 'rolling_base_volume', \n",
    "                                         'rolling_quote_volume', 'open_time', 'close_time', 'first_ID', 'last_ID', \n",
    "                                         'count', 'base_asset', 'quote_asset', 'rolling_base_quote_volume', \n",
    "                                         'USDT_open', 'USDT_high', 'USDT_low', 'USDT_price', \n",
    "                                         'rolling_USDT_base_volume', 'rolling_USDT_quote_volume', \n",
    "                                         'USDT_bid_price', 'USDT_ask_price', 'USDT_bid_volume', \n",
    "                                         'USDT_ask_volume', 'USDT_price_change', 'USDT_price_change_percent', \n",
    "                                         'is_shorted']].values\n",
    "    conversion_table_swapped['symbol'] = \\\n",
    "        conversion_table_swapped['base_asset'] + conversion_table_swapped['quote_asset']\n",
    "    conversion_table_swapped.loc[:, ['open', 'high', 'low', 'close', 'close_shifted', 'bid_price', 'ask_price', \n",
    "                                     'USDT_open', 'USDT_high', 'USDT_low', 'USDT_price', 'USDT_bid_price', \n",
    "                                     'USDT_ask_price', 'USDT_price_change', 'USDT_price_change_percent']] = \\\n",
    "        1 / conversion_table_swapped.loc[:, ['open', 'high', 'low', 'close', 'close_shifted', \n",
    "                                             'bid_price', 'ask_price', 'USDT_open', 'USDT_high', \n",
    "                                             'USDT_low', 'USDT_price', 'USDT_bid_price', 'USDT_ask_price', \n",
    "                                             'USDT_price_change', 'USDT_price_change_percent']].astype(float)\n",
    "    conversion_table_swapped['is_shorted'] = True\n",
    "\n",
    "    conversion_table = concat([conversion_table, conversion_table_swapped], join='outer', axis='index')\n",
    "\n",
    "    traded_volume = conversion_table.groupby(by='base_asset').agg('sum')\n",
    "    traded_volume = traded_volume['rolling_USDT_base_volume']\n",
    "    conversion_table['rolling_traded_volume'] = \\\n",
    "        conversion_table.apply(lambda x: traded_volume.loc[x['base_asset']], axis='columns')\n",
    "    traded_bid_volume = conversion_table.groupby(by='base_asset').agg('sum')\n",
    "    traded_bid_volume = traded_bid_volume['USDT_bid_volume']\n",
    "    conversion_table['traded_bid_volume'] = \\\n",
    "        conversion_table.apply(lambda x: traded_bid_volume.loc[x['base_asset']], axis='columns')\n",
    "    traded_ask_volume = conversion_table.groupby(by='base_asset').agg('sum')\n",
    "    traded_ask_volume = traded_ask_volume['USDT_ask_volume']\n",
    "    conversion_table['traded_ask_volume'] = \\\n",
    "        conversion_table.apply(lambda x: traded_ask_volume.loc[x['base_asset']], axis='columns')\n",
    "\n",
    "    conversion_table['importance'] = \\\n",
    "        conversion_table['rolling_USDT_base_volume'] / conversion_table['rolling_traded_volume']\n",
    "\n",
    "    conversion_table['importance_weighted_price'] = \\\n",
    "        conversion_table['USDT_price'] * conversion_table['importance']\n",
    "    conversion_table['importance_weighted_bid_price'] = \\\n",
    "        conversion_table['USDT_bid_price'] * conversion_table['importance']\n",
    "    conversion_table['importance_weighted_ask_price'] = \\\n",
    "        conversion_table['USDT_ask_price'] * conversion_table['importance']\n",
    "\n",
    "    importance_weighted_price = conversion_table.groupby(by='base_asset').agg('sum')\n",
    "    importance_weighted_price = importance_weighted_price['importance_weighted_price']\n",
    "    conversion_table['traded_price'] = \\\n",
    "        conversion_table.apply(lambda x: importance_weighted_price.loc[x['base_asset']], axis='columns')\n",
    "    importance_weighted_bid_price = conversion_table.groupby(by='base_asset').agg('sum')\n",
    "    importance_weighted_bid_price = importance_weighted_bid_price['importance_weighted_bid_price']\n",
    "    conversion_table['traded_bid_price'] = \\\n",
    "        conversion_table.apply(lambda x: importance_weighted_bid_price.loc[x['base_asset']], axis='columns')\n",
    "    importance_weighted_ask_price = conversion_table.groupby(by='base_asset').agg('sum')\n",
    "    importance_weighted_ask_price = importance_weighted_ask_price['importance_weighted_ask_price']\n",
    "    conversion_table['traded_ask_price'] = \\\n",
    "        conversion_table.apply(lambda x: importance_weighted_ask_price.loc[x['base_asset']], axis='columns')\n",
    "\n",
    "    conversion_table['bid_ask_change_percent'] = \\\n",
    "        ((conversion_table['ask_price'] - conversion_table['bid_price']) / conversion_table['ask_price'])\n",
    "    conversion_table['bid_ask_volume_percent'] = \\\n",
    "        (conversion_table['bid_volume'] / (conversion_table['bid_volume'] + conversion_table['ask_volume']))\n",
    "    conversion_table[['bid_ask_change_percent', 'bid_ask_volume_percent']] *= 100\n",
    "    conversion_table['traded_bid_ask_change_percent'] = \\\n",
    "        ((conversion_table['traded_ask_price'] - conversion_table['traded_bid_price']) / \\\n",
    "         conversion_table['traded_ask_price'])\n",
    "    conversion_table['traded_bid_ask_volume_percent'] = \\\n",
    "        (conversion_table['traded_bid_volume'] / (conversion_table['traded_bid_volume'] + \\\n",
    "                                                  conversion_table['traded_ask_volume']))\n",
    "    conversion_table[['traded_bid_ask_change_percent', 'traded_bid_ask_volume_percent']] *= 100\n",
    "\n",
    "    conversion_table = conversion_table[~conversion_table['is_shorted']]\n",
    "    if as_pair:\n",
    "        conversion_table = \\\n",
    "            conversion_table[['symbol', 'base_asset', 'quote_asset', 'is_shorted', 'price_change_percent', \n",
    "                              'weighted_average_price', 'open', 'high', 'low', 'close', 'close_shifted', \n",
    "                              'last_volume', 'bid_price', 'bid_volume', 'ask_price', 'ask_volume', \n",
    "                              'close_time', 'last_ID', 'count', 'rolling_base_volume', 'rolling_quote_volume', \n",
    "                              'importance', 'USDT_price_change_percent', 'USDT_open', \n",
    "                              'USDT_high', 'USDT_low', 'USDT_price', 'rolling_USDT_base_volume', \n",
    "                              'rolling_USDT_quote_volume', 'USDT_bid_price', 'USDT_ask_price', \n",
    "                              'USDT_bid_volume', 'USDT_ask_volume', 'rolling_traded_volume', \n",
    "                              'traded_bid_volume', 'traded_ask_volume', 'traded_price', \n",
    "                              'traded_bid_price', 'traded_ask_price', 'bid_ask_change_percent', \n",
    "                              'bid_ask_volume_percent', 'traded_bid_ask_change_percent', \n",
    "                              'traded_bid_ask_volume_percent']]\n",
    "    else:\n",
    "        conversion_table = \\\n",
    "            conversion_table[['base_asset', 'USDT_price_change_percent', 'close_time', 'last_ID', \n",
    "                              'count', 'rolling_traded_volume', 'traded_bid_volume', \n",
    "                              'traded_ask_volume', 'traded_price', 'traded_bid_price', \n",
    "                              'traded_ask_price', 'traded_bid_ask_change_percent', \n",
    "                              'traded_bid_ask_volume_percent']]\n",
    "        conversion_table['rolling_quote_volume'] = conversion_table['rolling_traded_volume'].copy()\n",
    "        conversion_table = \\\n",
    "            conversion_table.rename(columns={'USDT_price_change_percent': 'price_change_percent', \n",
    "                                             'rolling_traded_volume': 'rolling_base_volume', \n",
    "                                             'traded_bid_volume': 'bid_volume', \n",
    "                                             'traded_ask_volume': 'ask_volume', \n",
    "                                             'traded_price': 'close', \n",
    "                                             'traded_bid_price': 'bid_price', \n",
    "                                             'traded_ask_price': 'ask_price', \n",
    "                                             'traded_bid_ask_change_percent': 'bid_ask_change_percent', \n",
    "                                             'traded_bid_ask_volume_percent': 'bid_ask_volume_percent'})\n",
    "        conversion_table['symbol'] = conversion_table['base_asset'].copy()\n",
    "        conversion_table['quote_asset'] = conversion_table['base_asset'].copy()\n",
    "        df = conversion_table.groupby(by=['base_asset']).agg({'close_time': 'max', \n",
    "                                                              'last_ID': 'sum', \n",
    "                                                              'count': 'sum'})\n",
    "        conversion_table.loc[:, ['close_time', 'last_ID', 'count']] = \\\n",
    "            conversion_table.apply(lambda x: df.loc[x['base_asset']], axis='columns')\n",
    "        conversion_table = conversion_table.drop_duplicates(subset=['base_asset'], keep='first')\n",
    "        conversion_table = conversion_table.reset_index(drop=True)\n",
    "\n",
    "    conversion_table = conversion_table.sort_values(by='close_time')\n",
    "    conversion_table = conversion_table.reset_index(drop=True)\n",
    "    return conversion_table\n",
    "\n",
    "conversion_table = get_conversion_table(client=client, exchange_info=exchange_info, as_pair=False)\n",
    "conversion_table.sort_values(by='rolling_base_volume', ascending=False).reset_index(drop=True).head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbol = 'NEBL'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "crypto_output_log_1d = 'crypto_logs/crypto_output_log_1d.txt'\n",
    "df_1d = pd.read_csv(crypto_output_log_1d, header=[0, 1], index_col=0)\n",
    "df_1d.index = pd.DatetimeIndex(df_1d.index)\n",
    "df_1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "crypto_output_log_15s = 'crypto_logs/crypto_output_log_15s.txt'\n",
    "df_15s = pd.read_csv(crypto_output_log_15s, header=[0, 1], index_col=0)\n",
    "df_15s.index = pd.DatetimeIndex(df_15s.index)\n",
    "df_15s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_15s[symbol]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "crypto_output_log_1min = 'crypto_logs/crypto_output_log_1min.txt'\n",
    "df_1min = pd.read_csv(crypto_output_log_1min, header=[0, 1], index_col=0)\n",
    "df_1min.index = pd.DatetimeIndex(df_1min.index)\n",
    "df_1min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1min[symbol].tail(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "crypto_output_log_1h = 'crypto_logs/crypto_output_log_1h.txt'\n",
    "df_1h = pd.read_csv(crypto_output_log_1h, header=[0, 1], index_col=0)\n",
    "df_1h.index = pd.DatetimeIndex(df_1h.index)\n",
    "df_1h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1h[symbol].tail(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_1h.copy()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tz_local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "df = df_1h.copy()\n",
    "is_dst = time.localtime().tm_isdst\n",
    "timezone = time.tzname[is_dst]\n",
    "offset_ns = time.altzone if is_dst else time.timezone\n",
    "offset = (offset_ns / 60 / 60 * -1)\n",
    "df.index = df.index + pd.Timedelta(-is_dst, unit='h')\n",
    "df.index = df.index.tz_localize(tz=time.tzname[0])\n",
    "df.index = pd.DatetimeIndex(df.index, ambiguous='infer')\n",
    "df.index = df.index.tz_convert('UTC')\n",
    "df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.index[-1].dst()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "df.index.to_pydatetime()[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_1min.copy()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cryptocurrency.authentication import Cryptocurrency_authenticator\n",
    "from cryptocurrency.exchange import Cryptocurrency_exchange\n",
    "from cryptocurrency.conversion_table import get_conversion_table\n",
    "import pandas as pd\n",
    "\n",
    "authenticator = Cryptocurrency_authenticator(use_keys=False, testnet=False)\n",
    "client = authenticator.spot_client\n",
    "exchange = Cryptocurrency_exchange(client=client, directory='crypto_logs')\n",
    "exchange_info = exchange.info\n",
    "\n",
    "crypto_output_log_1min = 'crypto_logs/crypto_output_log_1min.txt'\n",
    "df_1min = pd.read_csv(crypto_output_log_1min, header=[0, 1], index_col=0)\n",
    "df_1min.index = pd.DatetimeIndex(df_1min.index)\n",
    "\n",
    "conversion_table = df_1min.copy()\n",
    "conversion_table.columns = conversion_table.columns.swaplevel(0, 1)\n",
    "conversion_table = conversion_table.drop(columns=['rolling_base_volume', \n",
    "                                                  'rolling_quote_volume'])\n",
    "conversion_table.columns = conversion_table.columns.swaplevel(0, 1)\n",
    "conversion_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cryptocurrency.conversion import convert_ohlcvs_from_pairs_to_assets\n",
    "\n",
    "new_conversion_table = convert_ohlcvs_from_pairs_to_assets(conversion_table, \n",
    "                                                           exchange_info)\n",
    "new_conversion_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_conversion_table['ETH']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyO4csgmdyxBlfVzNgUhkI0X",
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "crypto_bot",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
